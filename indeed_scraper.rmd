---
title: "Indeed Scraper"
Author: "Paul Britton"
output:
  html_document:
    highlight: pygments
    theme: cerulean
    toc: yes
  pdf_document:
    toc: yes
---


```{r clear_workspace, echo=FALSE}
rm(list = ls())
```


# Overview:

The following chunk of code scrapes job postings from indeed.ca and collects the results into a dataframe.  This code works as of 2018-04-08, however it may break given the "living" nature of the web & no serious effort has been made here to future-proof it.


## Load the libraries:

```{r setup, eval=TRUE, warning=FALSE, message=FALSE}
library(rvest)
library(RCurl)
library(knitr)
```





## Scrape the Details & Get the Full Summary

#### Getting the Details

Indeed.ca uses the "GET" request method, so we can directly manipulate the URL to get the data that we want.  We're going to iterate over our target cities and scrape the particulars for each job - this includes getting the links to each individual job-page so that we can also pull the full summary

#### Getting the full Summary

After the above is complete, we're going to iterate over all the links that we've collected, pull them, and grab the full job summary for each.  Note that it appears that jobs postings are sometimes removed, in which case, we will end up with an empty line in our dataframe.  We could probably do some cleaning in this step while downloading, but we'll leave that for down-stream as we don't want to assume the intentions of the end user.

Also, scraping the full summaries is a slow process as links need to be pulled one-by-one.  The user can elect to disable this feature, in which case the returned dataframe will have no values in the "summary.full" column.

```{r scrape_function, eval=TRUE}

scrape.jobs <- function(city.set,target.job,base.url,max.results,full.summary){
  
  #create a df to hold everything that we collect
  jobs.data <- data.frame(matrix(ncol = 7, nrow = 0))
  
  n <- c("city",
         "job.title",
         "company.name",
         "job.location",
         "summary.short",
         "links",
         "summary.full")
  
  
  colnames(jobs.data) <- n
  
  for (city in city.set){
    print(paste("Downloading data for: ", city))
  
    
    for (start in range(0,max.results,10)){
  
      url <- paste(base.url,"jobs?q=",target.job,"&l=",city,"&start=", start ,sep="")
      page <- read_html(url)
      Sys.sleep(1)
    
      #get the links
      links <- page %>% 
        html_nodes("div") %>%
        html_nodes(xpath = '//*[@data-tn-element="jobTitle"]') %>%
        html_attr("href")
      
      #get the job title
      job.title <- page %>% 
        html_nodes("div") %>%
        html_nodes(xpath = '//*[@data-tn-element="jobTitle"]') %>%
        html_attr("title")
    
      #get the company name
      company.name <- page %>% 
        html_nodes("span")  %>% 
        html_nodes(xpath = '//*[@class="company"]')  %>% 
        html_text() %>%
        trimws -> company.name 
    
      #get job location
      job.location <- page %>% 
        html_nodes("span") %>% 
        html_nodes(xpath = '//*[@class="location"]')%>% 
        html_text() %>%
        trimws -> job.location
      
      #get the short sumary
      summary.short <- page %>% 
        html_nodes("span")  %>% 
        html_nodes(xpath = '//*[@class="summary"]')  %>% 
        html_text() %>%
        trimws -> summary.short 
      
    } # for results
    
    #create a structure to hold our full summaries
    summary.full <- rep(NA, length(links))
    
    #fill in the job data
    job.city <- rep(city,length(links))
    
    #add a place-holder for the salary
    job.salary <- rep(0,length(links))
    
    
    #do we want the full summary?
    if (isTRUE(full.summary)){
    
      #iterate over the links that we collected
      for ( n in 1:length(links) ){
        
        #build the link
        link <- paste(base.url,links[n],sep="")
        
        #pull the link
        page <- read_html(link)
      
        #get the full summary
        s.full <- page %>%
         html_nodes("span")  %>% 
         html_nodes(xpath = '//*[@class="summary"]') %>% 
         html_text() %>%
         trimws -> s.full
      
        #check to make sure we got some data and if so, append it.
        #as expired postings return an empty var
        if (length(s.full) > 0 ){
            summary.full[n] = s.full  
            } 
      
        } # for links
    } #isTrue (summaryFull)
    
    
    #add the newly collected data to the jobs.data
    jobs.data <- rbind(jobs.data,data.frame(city,
                                            job.title,
                                            company.name,
                                            job.location,
                                            summary.short,
                                            links,
                                            summary.full))
    
 
  }
  return(jobs.data)
} 

```

## Collecting Some Data

Now we'll collect a bit of data and see what we get.  The function takes 5 arguments:

1. **city.set**: A vector of "city+region" pairings.
2. **target.job**: Target terms, separated by a "+"
3. **base.url**: The indeed url (.ca, .com)
4. **max.results**:  The max number of hits to be returned
5. **full.summary**:  Boolean.  Return full job summary

```{r grab_data, eval=TRUE}

city.set <- c("Ottawa+ON","Toronto+ON")
target.job <- "data+scientist"   
base.url <- "https://www.indeed.ca/"
max.results <- 50
full.summary <- TRUE

data <- scrape.jobs(city.set,target.job,base.url,max.results,full.summary)
```

We'll write it to a CSV that can be immortalized on github.

```{r write_data, eval=TRUE}
write.csv(data,file="indeed_test.csv")

```

Let's look at what we've got!  Note, I've ommitted some columns for plotting purposes.

```{r display, eval=TRUE}

kable(data[1:10,1:3])

```


